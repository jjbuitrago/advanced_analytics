{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "matplotlib.rc('text', usetex=True)\n",
    "matplotlib.rcParams['text.latex.preamble']=[r\"\\usepackage{amsmath}\"]\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "separable = False\n",
    "while not separable:\n",
    "    samples = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_informative=1, n_clusters_per_class=1, flip_y=-1)\n",
    "    red = samples[0][samples[1] == 0]\n",
    "    blue = samples[0][samples[1] == 1]\n",
    "    separable = any([red[:, k].max() < blue[:, k].min() or red[:, k].min() > blue[:, k].max() for k in range(2)])\n",
    "\n",
    "\n",
    "red_labels = np.zeros(len(red))\n",
    "blue_labels = np.ones(len(blue))\n",
    "\n",
    "labels = np.append(red_labels,blue_labels)\n",
    "inputs = np.concatenate((red,blue),axis=0)\n",
    "\n",
    "X_train, X_test, y_train,  y_test = train_test_split(\n",
    "    inputs, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(self.linear(x))\n",
    "        return outputs\n",
    "\n",
    "epochs = 200_000\n",
    "input_dim = 2 # Two inputs x1 and x2 \n",
    "output_dim = 1 # Two possible outputs\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = LogisticRegression(input_dim,output_dim)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "X_train, X_test = torch.Tensor(X_train),torch.Tensor(X_test)\n",
    "y_train, y_test = torch.Tensor(y_train),torch.Tensor(y_test)\n",
    "\n",
    "losses = []\n",
    "losses_test = []\n",
    "Iterations = []\n",
    "iter = 0\n",
    "for epoch in tqdm(range(int(epochs)),desc='Training Epochs'):\n",
    "    x = X_train\n",
    "    labels = y_train\n",
    "    optimizer.zero_grad() # Setting our stored gradients equal to zero\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(torch.squeeze(outputs), labels) # [200,1] -squeeze-> [200]\n",
    "    \n",
    "    loss.backward() # Computes the gradient of the given tensor w.r.t. graph leaves \n",
    "    \n",
    "    optimizer.step() # Updates weights and biases with the optimizer (SGD)\n",
    "    \n",
    "    iter+=1\n",
    "    if iter%10000==0:\n",
    "        # calculate Accuracy\n",
    "        with torch.no_grad():\n",
    "            # Calculating the loss and accuracy for the test dataset\n",
    "            correct_test = 0\n",
    "            total_test = 0\n",
    "            outputs_test = torch.squeeze(model(X_test))\n",
    "            loss_test = criterion(outputs_test, y_test)\n",
    "            \n",
    "            predicted_test = outputs_test.round().detach().numpy()\n",
    "            total_test += y_test.size(0)\n",
    "            correct_test += np.sum(predicted_test == y_test.detach().numpy())\n",
    "            accuracy_test = 100 * correct_test/total_test\n",
    "            losses_test.append(loss_test.item())\n",
    "            \n",
    "            # Calculating the loss and accuracy for the train dataset\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            total += y_train.size(0)\n",
    "            correct += np.sum(torch.squeeze(outputs).round().detach().numpy() == y_train.detach().numpy())\n",
    "            accuracy = 100 * correct/total\n",
    "            losses.append(loss.item())\n",
    "            Iterations.append(iter)\n",
    "            \n",
    "            print(f\"Iteration: {iter}. \\nTest - Loss: {loss_test.item()}. Accuracy: {accuracy_test}\")\n",
    "            print(f\"Train -  Loss: {loss.item()}. Accuracy: {accuracy}\\n\")\n",
    "\n",
    "\n",
    "def model_plot(model,X,y,title):\n",
    "    parm = {}\n",
    "    b = []\n",
    "    for name, param in model.named_parameters():\n",
    "        parm[name]=param.detach().numpy()  \n",
    "    \n",
    "    w = parm['linear.weight'][0]\n",
    "    b = parm['linear.bias'][0]\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y,cmap='jet')\n",
    "    u = np.linspace(X[:, 0].min(), X[:, 0].max(), 2)\n",
    "    plt.plot(u, (0.5-b-w[0]*u)/w[1])\n",
    "    plt.xlim(X[:, 0].min()-0.5, X[:, 0].max()+0.5)\n",
    "    plt.ylim(X[:, 1].min()-0.5, X[:, 1].max()+0.5)\n",
    "    plt.xlabel(r'$\\boldsymbol{x_1}$',fontsize=16) # Normally you can just add the argument fontweight='bold' but it does not work with latex\n",
    "    plt.ylabel(r'$\\boldsymbol{x_2}$',fontsize=16)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Train Data\n",
    "model_plot(model,X_train,y_train,'Train Data')\n",
    "\n",
    "# Test Dataset Results\n",
    "model_plot(model,X_test,y_test,'Test Data')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
