{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tyyrhgfg\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class for the loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self,df_x, df_y):\n",
    "    self.x_train=torch.tensor(df_x,dtype=torch.float32)\n",
    "    self.y_train=torch.tensor(df_y,dtype=torch.float32)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.y_train)\n",
    "  \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x_train[idx],self.y_train[idx] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to process the DF. This will be useful to process the test set in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df):\n",
    "    df = pd.get_dummies(df, columns = [\"customer_children\"], drop_first=True)\n",
    "    df = pd.get_dummies(df, columns = [\"customer_relationship\"], drop_first=True)\n",
    "    df[\"customer_gender\"] = df[\"customer_gender\"].replace(1,0).replace(2,1)\n",
    "\n",
    "    # Dropping irrelevant variables\n",
    "    df = df.drop([\"customer_postal_code\", \"customer_since_all\",\"customer_since_bank\", \"customer_occupation_code\", \"customer_education\", \"customer_birth_date\"], axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are target    1913\n",
      "dtype: int64 churn cases in 63697\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"data/train_month_3_with_target.csv\")\n",
    "\n",
    "# Dummifying some variables\n",
    "train_df = process_df(train_df)\n",
    "\n",
    "y=train_df\n",
    "y = y[[\"target\"]]\n",
    "X = train_df.drop([\"target\", \"client_id\"], axis = 1)\n",
    "\n",
    "print(f\"There are {np.sum(y)} churn cases in {len(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some EDA using **bal_savings_account**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f13d90092b0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.boxplot(x=\"bal_savings_account\", y=\"target\", data=train_df, orient=\"h\", palette={1:\"blue\", 0:\"green\"}, ax=ax)\n",
    "\n",
    "ax.get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle(\"bal_savings_account vs. Target\")\n",
    "\n",
    "color_patches = [\n",
    "    Patch(facecolor=\"blue\", label=\"default\"),\n",
    "    Patch(facecolor=\"green\", label=\"no default\")\n",
    "]\n",
    "ax.legend(handles=color_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the correlation between the output and all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_birth_date\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "correlations = {}\n",
    "best_cols = []\n",
    "\n",
    "for col in X.columns:\n",
    "    try:\n",
    "        corr = np.corrcoef(X[col], y[\"target\"])[1,0]\n",
    "        if np.abs(corr) > 0.08:\n",
    "            correlations[col] = corr\n",
    "            best_cols.append(col)\n",
    "    except:\n",
    "        print(col)\n",
    "\n",
    "with open('json_data.json', 'w') as outfile:\n",
    "    json.dump(correlations, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how I used to to id before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put it in the loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split( X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "training_set=MyDataset(X_train.values, y_train.values)\n",
    "validation_set=MyDataset(X_val.values, y_val.values)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True, num_workers=2)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model and the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__()\n",
    "        # # Define sigmoid activation and first layer\n",
    "        self.linear = torch.nn.Linear(input_size, 1)\n",
    "        self.Function = nn.LogSigmoid()\n",
    "\n",
    "    # def forward(self, inputs):\n",
    "    #     return self.Function(inputs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.Function(self.linear(inputs))\n",
    "\n",
    "\n",
    "model = Net(X_train.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece of code is useful to test the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3187493681907654\n",
      "0.22621630132198334\n"
     ]
    }
   ],
   "source": [
    "## Testing the loss\n",
    "f = 1\n",
    "for i, data in enumerate(training_loader):\n",
    "    # Every data instance is an input + label pair\n",
    "    inputs, labels = data\n",
    "    outputs = model(inputs)\n",
    "    # print(outputs.view(5), labels)\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss.item())\n",
    "    f += 1\n",
    "    if f == 3:\n",
    "        break\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the training and the prediction part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 382.70996875\n",
      "  batch 2000 loss: 0.03\n",
      "  batch 3000 loss: 0.02475\n",
      "  batch 4000 loss: 0.0295\n",
      "  batch 5000 loss: 0.035\n",
      "  batch 6000 loss: 0.02925\n",
      "  batch 7000 loss: 0.02875\n",
      "  batch 8000 loss: 0.03475\n",
      "  batch 9000 loss: 0.03025\n",
      "  batch 10000 loss: 0.03125\n",
      "LOSS train 0.03125 valid 0.0298230592161417\n",
      "model_20220404_175704_0\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.031\n",
      "  batch 2000 loss: 0.0285\n",
      "  batch 3000 loss: 0.02775\n",
      "  batch 4000 loss: 0.03175\n",
      "  batch 5000 loss: 0.03075\n",
      "  batch 6000 loss: 0.0305\n",
      "  batch 7000 loss: 0.02825\n",
      "  batch 8000 loss: 0.0275\n",
      "  batch 9000 loss: 0.03175\n",
      "  batch 10000 loss: 0.03425\n",
      "LOSS train 0.03425 valid 0.0298230592161417\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.0305\n",
      "  batch 2000 loss: 0.0315\n",
      "  batch 3000 loss: 0.02975\n",
      "  batch 4000 loss: 0.0285\n",
      "  batch 5000 loss: 0.02825\n",
      "  batch 6000 loss: 0.03475\n",
      "  batch 7000 loss: 0.03125\n",
      "  batch 8000 loss: 0.02825\n",
      "  batch 9000 loss: 0.02725\n",
      "  batch 10000 loss: 0.031\n",
      "LOSS train 0.031 valid 0.0298230592161417\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.034\n",
      "  batch 2000 loss: 0.02925\n",
      "  batch 3000 loss: 0.03175\n",
      "  batch 4000 loss: 0.02925\n",
      "  batch 5000 loss: 0.03275\n",
      "  batch 6000 loss: 0.029\n",
      "  batch 7000 loss: 0.02725\n",
      "  batch 8000 loss: 0.03025\n",
      "  batch 9000 loss: 0.0315\n",
      "  batch 10000 loss: 0.02775\n",
      "LOSS train 0.02775 valid 0.0298230592161417\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.0315\n",
      "  batch 2000 loss: 0.02875\n",
      "  batch 3000 loss: 0.033\n",
      "  batch 4000 loss: 0.03075\n",
      "  batch 5000 loss: 0.03075\n",
      "  batch 6000 loss: 0.03275\n",
      "  batch 7000 loss: 0.02325\n",
      "  batch 8000 loss: 0.03025\n",
      "  batch 9000 loss: 0.0315\n",
      "  batch 10000 loss: 0.0305\n",
      "LOSS train 0.0305 valid 0.0298230592161417\n",
      "EPOCH 6:\n",
      "  batch 1000 loss: 0.0315\n",
      "  batch 2000 loss: 0.03425\n",
      "  batch 3000 loss: 0.027\n",
      "  batch 4000 loss: 0.03225\n",
      "  batch 5000 loss: 0.0325\n",
      "  batch 6000 loss: 0.03075\n",
      "  batch 7000 loss: 0.02825\n",
      "  batch 8000 loss: 0.02625\n",
      "  batch 9000 loss: 0.03025\n",
      "  batch 10000 loss: 0.029\n",
      "LOSS train 0.029 valid 0.0298230592161417\n",
      "EPOCH 7:\n",
      "  batch 1000 loss: 0.031\n",
      "  batch 2000 loss: 0.034\n",
      "  batch 3000 loss: 0.02675\n",
      "  batch 4000 loss: 0.03325\n",
      "  batch 5000 loss: 0.03\n",
      "  batch 6000 loss: 0.032\n",
      "  batch 7000 loss: 0.0255\n",
      "  batch 8000 loss: 0.0285\n",
      "  batch 9000 loss: 0.028\n",
      "  batch 10000 loss: 0.03\n",
      "LOSS train 0.03 valid 0.0298230592161417\n",
      "EPOCH 8:\n",
      "  batch 1000 loss: 0.03075\n",
      "  batch 2000 loss: 0.031\n",
      "  batch 3000 loss: 0.03\n",
      "  batch 4000 loss: 0.02975\n",
      "  batch 5000 loss: 0.03125\n",
      "  batch 6000 loss: 0.03225\n",
      "  batch 7000 loss: 0.03\n",
      "  batch 8000 loss: 0.03025\n",
      "  batch 9000 loss: 0.0245\n",
      "  batch 10000 loss: 0.03325\n",
      "LOSS train 0.03325 valid 0.0298230592161417\n",
      "EPOCH 9:\n",
      "  batch 1000 loss: 0.03175\n",
      "  batch 2000 loss: 0.0305\n",
      "  batch 3000 loss: 0.02875\n",
      "  batch 4000 loss: 0.03125\n",
      "  batch 5000 loss: 0.02925\n",
      "  batch 6000 loss: 0.03525\n",
      "  batch 7000 loss: 0.02625\n",
      "  batch 8000 loss: 0.03025\n",
      "  batch 9000 loss: 0.02725\n",
      "  batch 10000 loss: 0.03125\n",
      "LOSS train 0.03125 valid 0.0298230592161417\n",
      "EPOCH 10:\n",
      "  batch 1000 loss: 0.03175\n",
      "  batch 2000 loss: 0.03325\n",
      "  batch 3000 loss: 0.0315\n",
      "  batch 4000 loss: 0.03075\n",
      "  batch 5000 loss: 0.03225\n",
      "  batch 6000 loss: 0.03125\n",
      "  batch 7000 loss: 0.0235\n",
      "  batch 8000 loss: 0.02775\n",
      "  batch 9000 loss: 0.032\n",
      "  batch 10000 loss: 0.0285\n",
      "LOSS train 0.0285 valid 0.0298230592161417\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_one_epoch(epoch_index): #, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number) #, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = criterion(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        print('model_{}_{}'.format(timestamp, epoch_number))\n",
    "\n",
    "    epoch_number += 1\n",
    "\n",
    "# y.to_csv(f\"{timestamp}_train.csv\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19368/1868732702.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[\"PROB\"] = train_data.squeeze().data.detach().numpy()\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"data/test_month_3.csv\")\n",
    "\n",
    "test_df = process_df(test_df)\n",
    "\n",
    "df_pred = pd.DataFrame()\n",
    "df_pred[\"ID\"] = test_df[\"client_id\"]\n",
    "\n",
    "X_test = test_df.drop([\"client_id\"], axis = 1)\n",
    "\n",
    "# X_test = X_test[best_cols]\n",
    "\n",
    "new_data = torch.tensor(X_test.values).type(torch.FloatTensor)\n",
    "old_data = torch.tensor(X.values).type(torch.FloatTensor)\n",
    "with torch.no_grad():\n",
    "    prediction = model(new_data)\n",
    "    train_data = model(old_data)\n",
    "\n",
    "df_pred[\"PROB\"] = prediction.squeeze().data.detach().numpy()\n",
    "y[\"PROB\"] = train_data.squeeze().data.detach().numpy()\n",
    "\n",
    "df_pred.to_csv(f\"{timestamp}_attempt.csv\", index = None)\n",
    "# y.to_csv(f\"{timestamp}_target.csv\", index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3858, 38)\n",
      "(3858, 1) target    1286\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "samplingstrat=0.5\n",
    "rus= RandomUnderSampler(sampling_strategy=samplingstrat,random_state=0)\n",
    "X_train_under, y_train_under=rus.fit_resample(X_train,y_train)\n",
    "\n",
    "print(X_train_under.shape)\n",
    "print(y_train_under.shape,y_train_under.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19368/3226531153.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  Rdfclf.fit(X_train_under,y_train_under)\n"
     ]
    }
   ],
   "source": [
    "Rdfclf= RandomForestClassifier(n_estimators=200,random_state=20)\n",
    "Rdfclf.fit(X_train_under,y_train_under)\n",
    "\n",
    "\n",
    "y_pred_under=Rdfclf.predict(X_test)\n",
    "\n",
    "# print(confusion_matrix(y_test,y_pred_under))\n",
    "# print(accuracy_score(y_test,y_pred_under))\n",
    "# print(classification_report(y_test,y_pred_under))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_under"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
