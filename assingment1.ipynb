{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self,df_x, df_y):\n",
    "    self.x_train=torch.tensor(df_x,dtype=torch.float32)\n",
    "    self.y_train=torch.tensor(df_y,dtype=torch.float32)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.y_train)\n",
    "  \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x_train[idx],self.y_train[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train_month_3_with_target.csv\")\n",
    "\n",
    "train_df = train_df.dropna(axis = 1)\n",
    "\n",
    "y=train_df[\"target\"]\n",
    "X = train_df.drop([\"target\", \"client_id\"], axis = 1)\n",
    "# X = X.drop([\"customer_since_all\",\"customer_since_bank\",\"customer_birth_date\", \"customer_children\",\"customer_relationship\"], axis = 1) # For now\n",
    "for col in [\"customer_since_all\",\"customer_since_bank\",\"customer_birth_date\", \"customer_children\",\"customer_relationship\"]:\n",
    "    try:\n",
    "        X = X.drop(col, axis = 1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split( X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "training_set=MyDataset(X_train.values, y_train.values)\n",
    "validation_set=MyDataset(X_val.values, y_val.values)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=5, shuffle=True, num_workers=2)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=5, shuffle=False, num_workers=2)\n",
    "\n",
    "epochs = 200000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, x_train_shape):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        # An affine operation: y = Wx + b de tipo todos contra todos\n",
    "        self.Layer_1 = nn.Linear(x_train_shape, 50) \n",
    "        self.Layer_2 = nn.Linear(50, 15)\n",
    "        self.Layer_Output = nn.Linear(15, 1)  \n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.Tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = self.Layer_1(inputs)\n",
    "        inputs = self.Tanh(inputs)\n",
    "        inputs = self.Layer_2(inputs)\n",
    "        return self.Layer_Output(inputs)\n",
    "\n",
    "model = LogisticRegression(X_train.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the loss\n",
    "\n",
    "for i, data in enumerate(training_loader):\n",
    "    # Every data instance is an input + label pair\n",
    "    inputs, labels = data\n",
    "    outputs = model(inputs)\n",
    "    # print(outputs.view(5), labels)\n",
    "    try:\n",
    "        loss = criterion(outputs.view(5), labels)\n",
    "    except RuntimeError:\n",
    "        continue\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index): #, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        try:\n",
    "            loss = criterion(outputs.view(5), labels)\n",
    "        except RuntimeError:\n",
    "            continue\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 0.029134452139650876\n",
      "  batch 2000 loss: 0.030744436903374833\n",
      "  batch 3000 loss: 0.03185657792591223\n",
      "  batch 4000 loss: 0.031464069919892826\n",
      "  batch 5000 loss: 0.03182176002581206\n",
      "  batch 6000 loss: 0.029913601609129897\n",
      "  batch 7000 loss: 0.025746806906390318\n",
      "  batch 8000 loss: 0.030043655693621985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juanjosebuitrago/.local/lib/python3.9/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.030043655693621985 valid 0.029722364619374275\n",
      "model_20220328_193353_0\n",
      "EPOCH 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juanjosebuitrago/.local/lib/python3.9/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1000 loss: 0.025311858922810616\n",
      "  batch 2000 loss: 0.029652970571271333\n",
      "  batch 3000 loss: 0.030969709305782942\n",
      "  batch 4000 loss: 0.033009140710921515\n",
      "  batch 5000 loss: 0.02886730037486757\n",
      "  batch 6000 loss: 0.026718309061016157\n",
      "  batch 7000 loss: 0.03265932775947067\n",
      "  batch 8000 loss: 0.02767199327852643\n",
      "LOSS train 0.02767199327852643 valid 0.029239526018500328\n",
      "model_20220328_193353_1\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.03520518171855395\n",
      "  batch 2000 loss: 0.029846433699193993\n",
      "  batch 3000 loss: 0.030804903733351693\n",
      "  batch 4000 loss: 0.02799236851220371\n",
      "  batch 5000 loss: 0.028910672497313102\n",
      "  batch 6000 loss: 0.028273425938797503\n",
      "  batch 7000 loss: 0.027265362517312496\n",
      "  batch 8000 loss: 0.025281679841929857\n",
      "LOSS train 0.025281679841929857 valid 0.029305964708328247\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.030850509532715478\n",
      "  batch 2000 loss: 0.029192629820520323\n",
      "  batch 3000 loss: 0.031378253233381885\n",
      "  batch 4000 loss: 0.02558596575033337\n",
      "  batch 5000 loss: 0.02786353680891625\n",
      "  batch 6000 loss: 0.027609237085058566\n",
      "  batch 7000 loss: 0.02915847727176242\n",
      "  batch 8000 loss: 0.031004819908068384\n",
      "LOSS train 0.031004819908068384 valid 0.02909916266798973\n",
      "model_20220328_193353_3\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.027381494518767736\n",
      "  batch 2000 loss: 0.031742511270504065\n",
      "  batch 3000 loss: 0.02682171097279246\n",
      "  batch 4000 loss: 0.030217200423209762\n",
      "  batch 5000 loss: 0.028158070086868228\n",
      "  batch 6000 loss: 0.028473244165549628\n",
      "  batch 7000 loss: 0.03236971341831713\n",
      "  batch 8000 loss: 0.026117969724162634\n",
      "LOSS train 0.026117969724162634 valid 0.02908185124397278\n",
      "model_20220328_193353_4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number) #, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = criterion(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    # writer.add_scalars('Training vs. Validation Loss',\n",
    "    #                 { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "    #                 epoch_number + 1)\n",
    "    # writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        print(model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/test_month_3.csv\")\n",
    "\n",
    "df_pred = pd.DataFrame()\n",
    "df_pred[\"ID\"] = test_df[\"client_id\"]\n",
    "\n",
    "test_df = test_df.dropna(axis = 1)\n",
    "\n",
    "X_test = test_df.drop([\"client_id\"], axis = 1)\n",
    "# X = X.drop([\"customer_since_all\",\"customer_since_bank\",\"customer_birth_date\", \"customer_children\",\"customer_relationship\"], axis = 1) # For now\n",
    "for col in [\"customer_since_all\",\"customer_since_bank\",\"customer_birth_date\", \"customer_children\",\"customer_relationship\"]:\n",
    "    try:\n",
    "        X_test = X_test.drop(col, axis = 1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "new_data = torch.tensor(X_test.values).type(torch.FloatTensor)\n",
    "with torch.no_grad():\n",
    "    prediction = model(new_data)\n",
    "\n",
    "df_pred[\"PROB\"] = prediction.view(27300).data.detach().numpy()\n",
    "\n",
    "df_pred.to_csv(f\"{timestamp}_attempt.csv\", index = None)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
