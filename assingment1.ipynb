{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self,df_x, df_y):\n",
    "    self.x_train=torch.tensor(df_x,dtype=torch.float32)\n",
    "    self.y_train=torch.tensor(df_y,dtype=torch.float32)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.y_train)\n",
    "  \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x_train[idx],self.y_train[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train_month_3_with_target.csv\")\n",
    "\n",
    "train_df = train_df.dropna(axis = 1)\n",
    "\n",
    "y=train_df\n",
    "y = y[[\"target\"]]\n",
    "X = train_df.drop([\"target\", \"client_id\"], axis = 1)\n",
    "# X = X.drop([\"customer_since_all\",\"customer_since_bank\",\"customer_birth_date\", \"customer_children\",\"customer_relationship\"], axis = 1) # For now\n",
    "for col in [\"customer_since_all\",\"customer_since_bank\",\"customer_birth_date\", \"customer_children\",\"customer_relationship\"]:\n",
    "    try:\n",
    "        X = X.drop(col, axis = 1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split( X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "training_set=MyDataset(X_train.values, y_train.values)\n",
    "validation_set=MyDataset(X_val.values, y_val.values)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True, num_workers=2)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__()\n",
    "        # # An affine operation: y = Wx + b de tipo todos contra todos\n",
    "        self.Layer_1 = nn.Linear(input_size, 1)\n",
    "        # self.Layer_2 = nn.Linear(20, 15)\n",
    "        \n",
    "        \n",
    "        # # Define sigmoid activation and softmax output \n",
    "        self.Function = nn.Sigmoid()\n",
    "        # self.linear = torch.nn.Linear(input_size, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.Layer_1(self.Function(inputs))\n",
    "\n",
    "model = Net(X_train.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the loss\n",
    "\n",
    "for i, data in enumerate(training_loader):\n",
    "    # Every data instance is an input + label pair\n",
    "    inputs, labels = data\n",
    "    outputs = model(inputs)\n",
    "    # print(outputs.view(5), labels)\n",
    "    loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index): #, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            # print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "LOSS train 0.028775623555459706 valid 0.02901441603899002\n",
      "model_20220329_120348_0\n",
      "EPOCH 2:\n",
      "LOSS train 0.03520748628765637 valid 0.0400262288749218\n",
      "EPOCH 3:\n",
      "LOSS train 0.034260290757730474 valid 0.029625317081809044\n",
      "EPOCH 4:\n",
      "LOSS train 0.035832407835012646 valid 0.035774219781160355\n",
      "EPOCH 5:\n",
      "LOSS train 0.0330128401989316 valid 0.02934136614203453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6160/153960872.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[\"PROB\"] = train_data.squeeze().data.detach().numpy()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number) #, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = criterion(voutputs.squeeze(), vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    # writer.add_scalars('Training vs. Validation Loss',\n",
    "    #                 { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "    #                 epoch_number + 1)\n",
    "    # writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        print('model_{}_{}'.format(timestamp, epoch_number))\n",
    "\n",
    "    epoch_number += 1\n",
    "\n",
    "test_df = pd.read_csv(\"data/test_month_3.csv\")\n",
    "\n",
    "df_pred = pd.DataFrame()\n",
    "df_pred[\"ID\"] = test_df[\"client_id\"]\n",
    "\n",
    "test_df = test_df.dropna(axis = 1)\n",
    "\n",
    "X_test = test_df.drop([\"client_id\"], axis = 1)\n",
    "# X = X.drop([\"customer_since_all\",\"customer_since_bank\",\"customer_birth_date\", \"customer_children\",\"customer_relationship\"], axis = 1) # For now\n",
    "for col in [\"customer_since_all\",\"customer_since_bank\",\"customer_birth_date\", \"customer_children\",\"customer_relationship\"]:\n",
    "    try:\n",
    "        X_test = X_test.drop(col, axis = 1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "new_data = torch.tensor(X_test.values).type(torch.FloatTensor)\n",
    "old_data = torch.tensor(X.values).type(torch.FloatTensor)\n",
    "with torch.no_grad():\n",
    "    prediction = model(new_data)\n",
    "    train_data = model(old_data)\n",
    "\n",
    "df_pred[\"PROB\"] = prediction.squeeze().data.detach().numpy()\n",
    "y[\"PROB\"] = train_data.squeeze().data.detach().numpy()\n",
    "\n",
    "df_pred.to_csv(f\"{timestamp}_attempt.csv\", index = None)\n",
    "y.to_csv(f\"{timestamp}_train.csv\", index = None)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
